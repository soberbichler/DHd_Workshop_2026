{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/DHd_Workshop_2026/blob/main/Inference_API_Huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL56CWXjl3vT"
      },
      "source": [
        "#Running LLMs via HuggingFace\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJA5bCH8pL9f"
      },
      "source": [
        "## Requirements for Hugging Face Inference API\n",
        "\n",
        "* Free Hugging Face account - No paid subscription required (free tier available)\n",
        "* Read access token - Generate a token from your account settings\n",
        "* No payment method required - Free tier includes generous API limits\n",
        "\n",
        "## Authentication Setup\n",
        "\n",
        "* Create your access token at huggingface.co/settings/tokens (you will be given an API as part of the workshop)\n",
        "* Token can have \"Read\" permissions (no write needed for inference)\n",
        "* Set token in your code or environment as HF_TOKEN:\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Get Started\n",
        "\n",
        "1.   Paste the Wikipedia article on Machine Learning under \"Article\" and ask the model to explain what Machine Learning is to test if this notebook works as intended. Compare your answer with your neighbor to see if the output is consistent.\n",
        "\n",
        "2.   Add the narrative event detection prompt and an newspaper article (German, French, or English), all availble in our folder. Compare models against each other. Compare models outputs against \"ideal\" results.\n",
        "\n",
        "3.   What are your conclusions? Can you improve the results by changing the prompt or model parameters?\n"
      ],
      "metadata": {
        "id": "6zP7IcR_KtOT"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoSgJNpkokBa"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import InferenceClient\n",
        "import json\n",
        "import time\n",
        "\n",
        "# CONFIGURATION\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "MODEL = \"allenai/Olmo-3.1-32B-Instruct\"  # You can change model here (allenai/Olmo-3.1-32B-Instruct; meta-llama/Llama-3.3-70B-Instruct; swiss-ai/Apertus-70B-Instruct-2509)\n",
        "\n",
        "# PROMPT\n",
        "SYSTEM_PROMPT = \"\"\"Test\"\"\"\n",
        "\n",
        "# ARTICLE\n",
        "ARTICLE = \"\"\"Add historical newspaper article here\"\"\"\n",
        "\n",
        "\n",
        "# MAIN FUNCTION\n",
        "def extract_events(article_text, model_name=MODEL):\n",
        "    \"\"\"\n",
        "    Extract events from article using HF Inference API\n",
        "\n",
        "    Args:\n",
        "        article_text: The newspaper article text\n",
        "        model_name: HuggingFace model to use\n",
        "\n",
        "    Returns:\n",
        "        Extracted events as string (JSON format)\n",
        "    \"\"\"\n",
        "    print(f\"Starting event extraction...\")\n",
        "    print(f\"Article length: {len(article_text)} characters\")\n",
        "    print(f\"Using model: {model_name}\")\n",
        "\n",
        "    # Initialize client\n",
        "    client = InferenceClient(token=HF_TOKEN)\n",
        "\n",
        "    # Prepare messages with proper system prompt\n",
        "    messages = [\n",
        "        {\n",
        "            \"role\": \"system\",\n",
        "            \"content\": SYSTEM_PROMPT\n",
        "        },\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": f\"## EXTRACT FROM THIS TEXT:\\n\\n{article_text}\\n\\nReturn only JSON array.\"\n",
        "        }\n",
        "    ]\n",
        "\n",
        "    # Call API\n",
        "    print(\"Calling Inference API...\")\n",
        "    start_time = time.time()\n",
        "\n",
        "    try:\n",
        "        response = client.chat_completion(\n",
        "            messages=messages,\n",
        "            model=model_name,\n",
        "            max_tokens=4000,\n",
        "            temperature=0.0\n",
        "        )\n",
        "\n",
        "        elapsed = time.time() - start_time\n",
        "        print(f\"API call completed in {elapsed:.2f} seconds\")\n",
        "\n",
        "        # Extract result\n",
        "        result = response.choices[0].message.content\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def main():\n",
        "    # Extract events\n",
        "    events = extract_events(ARTICLE)\n",
        "\n",
        "    if events:\n",
        "        print(events)\n",
        "\n",
        "        # Try to parse and count\n",
        "        try:\n",
        "            # Clean potential markdown\n",
        "            events_clean = events.replace(\"```json\", \"\").replace(\"```\", \"\").strip()\n",
        "            events_list = json.loads(events_clean)\n",
        "        except:\n",
        "            print(\"\\nCould not parse as JSON, but extraction completed.\")\n",
        "    else:\n",
        "        print(\"Extraction failed\")\n",
        "\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNP7N0cBjuK0mizgtdfdZOA",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}