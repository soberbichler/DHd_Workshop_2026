{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/soberbichler/DHd_Workshop_2026/blob/main/Job_Huggingface.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lL56CWXjl3vT"
      },
      "source": [
        "#Running LLM Jobs via HuggingFace\n",
        "\n",
        "For explanations on Hugginface https://huggingface.co/docs/huggingface_hub/guides/jobs\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tJA5bCH8pL9f"
      },
      "source": [
        "##Requirements for Hugging Face Jobs\n",
        "\n",
        "\n",
        "\n",
        "*   Hugging Face Pro account - A paid subscription is required to access job creation features\n",
        "*   Write access token - Generate a token with write permissions from your account settings\n",
        "*   Valid payment method - Jobs consume compute credits based on usage\n",
        "\n",
        "\n",
        "##Authentication Setup\n",
        "\n",
        "\n",
        "\n",
        "*   Create your access token at huggingface.co/settings/tokens (you will be given an API as part of the workshop)\n",
        "*   Ensure the token has \"Write\" permissions enabled\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uAqtsk-ns8f9"
      },
      "source": [
        "##Prepare your HF Job Script:\n",
        "\n",
        "This script creates a remote computational job on HuggingFace's infrastructure that loads a language model and answers a question. It uses `run_job` to spin up a GPU-enabled Docker container (PyTorch with CUDA), installs necessary Python packages (transformers, accelerate, etc.), then runs a Python script that loads a chosen model and defines the task."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Analyze a Dataset\n",
        "\n",
        "\n",
        "\n",
        "1.   Run the first cell and upload the dataset \"Earthquake_Articles_your_name.csv\" Add your name to the dataset as well as in the code.\n",
        "2.   Add the narrativ event detection prompt\n",
        "3.   Run the second cell to monitor the job. While the code is running, investigate the code. Can you explain what the code is doing?\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "9Jao3zcySJBN"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xoSgJNpkokBa"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import run_job, upload_file\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Upload CSV\n",
        "uploaded = files.upload()\n",
        "local_filename = list(uploaded.keys())[0]\n",
        "safe_filename = \"earthquake_articles_your_name.csv\"\n",
        "os.rename(local_filename, safe_filename)\n",
        "\n",
        "# Upload to HF\n",
        "from google.colab import userdata\n",
        "HF_TOKEN = userdata.get(\"HF_TOKEN\")\n",
        "upload_file(\n",
        "    path_or_fileobj=safe_filename,\n",
        "    path_in_repo=safe_filename,\n",
        "    repo_id=\"oberbics/jobs\",\n",
        "    repo_type=\"dataset\",\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "print(f\"Dataset uploaded: {safe_filename}\")\n",
        "\n",
        "# Submit job\n",
        "job = run_job(\n",
        "    image=\"pytorch/pytorch:2.6.0-cuda12.4-cudnn9-devel\",\n",
        "    command=[\n",
        "        \"bash\", \"-c\",\n",
        "        f\"\"\"\n",
        "        apt-get update && apt-get install -y wget &&\n",
        "        pip install -q \"transformers>=4.51.0\" accelerate bitsandbytes huggingface_hub pandas &&\n",
        "        wget -O earthquake_articles_your_name.csv https://huggingface.co/datasets/oberbics/jobs/resolve/main/earthquake_articles_your_name.csv &&\n",
        "        export PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True &&\n",
        "        export HF_TOKEN='{HF_TOKEN}' &&\n",
        "        python3 -c \"\n",
        "import os, torch, pandas as pd, datetime, re, json\n",
        "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'expandable_segments:True'\n",
        "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
        "from huggingface_hub import upload_file\n",
        "\n",
        "model_name = 'mistralai/Mistral-Small-24B-Instruct-2501'\n",
        "\n",
        "SYSTEM_PROMPT = '''Here prompt'''\n",
        "\n",
        "HF_TOKEN = os.environ.get('HF_TOKEN')\n",
        "\n",
        "df = pd.read_csv('earthquake_articles_your_name.csv', sep=';')\n",
        "print(f'Dataset loaded with {{len(df)}} rows')\n",
        "\n",
        "print('Loading model...')\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name, token=HF_TOKEN, fix_mistral_regex=True)\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "\n",
        "bnb_config = BitsAndBytesConfig(\n",
        "    load_in_4bit=True,\n",
        "    bnb_4bit_compute_dtype=torch.float16,\n",
        "    bnb_4bit_use_double_quant=True,\n",
        "    bnb_4bit_quant_type='nf4'\n",
        ")\n",
        "\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map='auto',\n",
        "    quantization_config=bnb_config,\n",
        "    low_cpu_mem_usage=True,\n",
        "    token=HF_TOKEN\n",
        ")\n",
        "print('Model loaded successfully!')\n",
        "\n",
        "def generate_extraction(model, tokenizer, text_to_analyze):\n",
        "    user_instruction = 'Extract agent/action/patient triples from this text:'\n",
        "    messages = [\n",
        "        {{'role': 'system', 'content': SYSTEM_PROMPT}},\n",
        "        {{'role': 'user', 'content': user_instruction + text_to_analyze[:5500]}}\n",
        "    ]\n",
        "    full_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    inputs = tokenizer(full_prompt, return_tensors='pt', truncation=True, max_length=5048).to(model.device)\n",
        "    input_length = inputs['input_ids'].shape[1]\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=5000,\n",
        "            do_sample=False,\n",
        "            pad_token_id=tokenizer.eos_token_id\n",
        "        )\n",
        "    generated_tokens = outputs[0][input_length:]\n",
        "    response = tokenizer.decode(generated_tokens, skip_special_tokens=True).strip()\n",
        "    return response\n",
        "\n",
        "def parse_json_response(response):\n",
        "    try:\n",
        "        cleaned = re.sub(r'```json|```', '', response).strip()\n",
        "        start = cleaned.find('[')\n",
        "        end = cleaned.rfind(']') + 1\n",
        "        if start != -1 and end > start:\n",
        "            cleaned = cleaned[start:end]\n",
        "        triples = json.loads(cleaned)\n",
        "        return triples, True\n",
        "    except Exception as e:\n",
        "        print(f'  JSON parse error: {{str(e)}}')\n",
        "        return [], False\n",
        "\n",
        "results = []\n",
        "parse_success = 0\n",
        "parse_failed = 0\n",
        "\n",
        "for idx, row in df.iterrows():\n",
        "    text = str(row.get('article_text', ''))\n",
        "    if pd.isna(text) or text.strip() in ['nan', '']:\n",
        "        print(f'Row {{idx}}: empty, skipping')\n",
        "        continue\n",
        "    print(f'Processing row {{idx}}...')\n",
        "    try:\n",
        "        response = generate_extraction(model, tokenizer, text)\n",
        "        triples, success = parse_json_response(response)\n",
        "\n",
        "        if success:\n",
        "            parse_success += 1\n",
        "            print(f'  Extracted {{len(triples)}} triples')\n",
        "            result_row = row.to_dict()\n",
        "            result_row.update({{\n",
        "                'llm_raw_response': response[:3000],\n",
        "                'triples_json': json.dumps(triples, ensure_ascii=False),\n",
        "                'triple_count': len(triples),\n",
        "                'processed_row_index': idx,\n",
        "                'model_used': model_name\n",
        "            }})\n",
        "            results.append(result_row)\n",
        "        else:\n",
        "            parse_failed += 1\n",
        "            result_row = row.to_dict()\n",
        "            result_row.update({{\n",
        "                'llm_raw_response': response[:3000],\n",
        "                'triples_json': 'PARSE_ERROR',\n",
        "                'triple_count': 0,\n",
        "                'processed_row_index': idx,\n",
        "                'model_used': model_name\n",
        "            }})\n",
        "            results.append(result_row)\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f'  Error: {{str(e)}}')\n",
        "        parse_failed += 1\n",
        "        result_row = row.to_dict()\n",
        "        result_row.update({{\n",
        "            'llm_raw_response': 'ERROR',\n",
        "            'triples_json': f'ERROR: {{str(e)}}',\n",
        "            'triple_count': 0,\n",
        "            'processed_row_index': idx,\n",
        "            'model_used': model_name\n",
        "        }})\n",
        "        results.append(result_row)\n",
        "\n",
        "total = parse_success + parse_failed\n",
        "if total > 0:\n",
        "    print('='*50)\n",
        "    print(f'PARSE SUCCESS: {{parse_success}}/{{total}} ({{parse_success/total*100:.1f}}%) | Failed: {{parse_failed}}/{{total}}')\n",
        "    print('='*50)\n",
        "\n",
        "output_df = pd.DataFrame(results)\n",
        "output_df.to_csv('output_triples.csv', index=False, sep=',', quoting=1)\n",
        "print(f'Saved {{len(output_df)}} triple rows to CSV')\n",
        "\n",
        "timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
        "filename = f'llm_triples_{{timestamp}}.csv'\n",
        "try:\n",
        "    upload_file(\n",
        "        path_or_fileobj='output_triples.csv',\n",
        "        path_in_repo=filename,\n",
        "        repo_id='oberbics/jobs',\n",
        "        repo_type='dataset',\n",
        "        token=HF_TOKEN,\n",
        "        commit_message=f'Mistral agent/action/patient extraction - {{timestamp}}'\n",
        "    )\n",
        "    print(f'SUCCESS: https://huggingface.co/datasets/oberbics/jobs/resolve/main/{{filename}}')\n",
        "except Exception as e:\n",
        "    print(f'Upload failed: {{str(e)}}')\n",
        "\n",
        "print(f'Job complete. Processed {{len(df)}} articles -> {{len(output_df)}} triples')\n",
        "\"\n",
        "        \"\"\"\n",
        "    ],\n",
        "    flavor=\"a100-large\",\n",
        "    timeout=\"4h\",\n",
        "    env={\"HUGGINGFACE_TOKEN\": HF_TOKEN}\n",
        ")\n",
        "\n",
        "print(f\"Job submitted! ID: {job.id}\")\n",
        "print(f\"Monitor at: https://huggingface.co/jobs/oberbics/{job.id}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9EAgsLe5dh0Y"
      },
      "outputs": [],
      "source": [
        "from huggingface_hub import inspect_job, fetch_job_logs\n",
        "import time\n",
        "\n",
        "# Poll job status until it's done\n",
        "while True:\n",
        "    status = inspect_job(job_id=job.id).status.stage\n",
        "    print(f\"Job status: {status}\")\n",
        "    if status in (\"COMPLETED\", \"ERROR\"):\n",
        "        break\n",
        "    time.sleep(10)\n",
        "\n",
        "# Fetch logs after completion\n",
        "print(\"\\n=== Job logs ===\")\n",
        "logs = list(fetch_job_logs(job_id=job.id))\n",
        "for line in logs:\n",
        "    print(line)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNJboOpIN9U2EfxNC/bljH/",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}